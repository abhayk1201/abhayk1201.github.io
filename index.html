<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
	<meta name=viewport content='width=800'>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
      /* Color scheme stolen from Sergey Karayev */
      a {
      color: #1772d0;
      text-decoration:none;
      }
      a:focus, a:hover {
      color: #f09228;
      text-decoration:none;
      }
      body,td,th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px
      }
      strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px;
      }
      heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22 px;
      }
      papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
      }
      name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
      }
	.fade {
	   transition: opacity .2s ease-in-out;
	   -moz-transition: opacity .2s ease-in-out;
	   -webkit-transition: opacity .2s ease-in-out;
	   }
	  
	img {
	    display: inline;
	    margin: 0 auto;
	    width: 100%;
	}
   .image-cropper {
      width: 250px;
      height: 250px;
      position: relative;
      overflow: hidden;
      border-radius: 50%;
  }
    </style>
    <link rel="icon" type="image" href="img/logo.jpg">
    <title>Abhay Kumar</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
    <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
      <tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="67%" valign="middle">
                <p align="center">
                  <name>Abhay Kumar</name>
                  </font>
                <p align>I am currently working at <a href="http://www.samsung.com/in/aboutsamsung/samsungelectronics/india/rnd/">Samsung R&D Institite- India, Bangalore.</a> I am primarily interested in Deep Learning and its application to Computer Vision and Natural Language Processing. 

                                  <br><br>
                                  I am currently working on Bixby - Artificial Intelligence based Smart Assistant, using deep learning technologies.
                  <br><br>
                   Prior to that, I did my undergraduation from <a href="https://www.iitk.ac.in/">Indian Institute of Technology Kanpur</a> with a major in Electrical Engineering.
                 
                <p align=center>
<a href="mailto:abykumar12011@gmail.com">Email</a> &nbsp/&nbsp
<a href="https://www.linkedin.com/in/abhaykumar3/">LinkedIn</a> &nbsp/&nbsp
<a href="files/TODO.pdf">Resume</a>
<!--<a href="http://github.com/EnayatUllah">Github</a> &nbsp/&nbsp
<a href="http://facebook.com/shreesh.ladha">Facebook</a>-->
<!--<a href="resume.pdf">One-page Resume</a>-->
<!-- &nbsp/&nbsp -->
<!-- <a href="http://www.goodreads.com/in/enayatullah/">Goodreads</a>    -->
<!-- <a href="http://www.linkedin.com/in/enayatullah/"> LinkedIn </a> -->
                </p>
              </td>
              <td width="33%"><img class="image-cropper" src="img/abhay.jpg"></td>
            </tr>
          </table>
            

           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading style="font-size:22px">Publications</heading>
              </td>
            </tr>
			<tr >
            
                    <td width="30%"><img id="img-opt" src="img/paper_2.png" alt="project_img" width="160" style="border-style: none">
                    </td>
        
                        <td valign="top" width="70%">
                        <p><a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1811.pdf">
            <papertitle>Speech Emotion Recognition Using Spectrogram & Phoneme Embedding</papertitle></a><br>
                        <em>Interspeech 2018, Hyderabad </em><br>  
                        <p style="text-align:justify">This paper proposes a speech emotion recognition method based on phoneme sequence and spectrogram. Both phoneme sequence and spectrogram retain emotion contents of speech which is missed if the speech is converted into text. We performed various experiments with different kinds of deep neural networks with phoneme and spectrogram as inputs. Three of those network architectures are presented here that helped to achieve better accuracy when compared to the state-of-the-art methods on benchmark dataset. A phoneme and spectrogram combined CNN model proved to be most accurate in recognizing emotions on IEMOCAP data. We achieved more than 4% increase in overall accuracy and average class accuracy as compared to the existing state-of-the-art methods.          
                        <p></p>
                        <a href="https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1811.pdf">paper link</a> | <a href="files/paper2.pdf">pdf</a>
                        </a> </p>
                        </td>
                    </tr>
			
			<tr >
            
                    <td width="30%"><img id="img-opt" src="img/paper_2.png" alt="project_img" width="160" style="border-style: none">
                    </td>
        
                        <td valign="top" width="70%">
                        <p><a href="">
            <papertitle>Exploiting SIFT Descriptor for Rotation Invariant Convolutional Neural Network</papertitle></a><br>
                        <em>15th IEEE India Council International Conference (INDICON 2018)  [ACCEPTED] </em><br>  
                        <p style="text-align:justify"> This paper  presents a novel approach to exploit the  distinctive  invariant  features  in  convolutional  neural network.  The  proposed  CNN  model  uses  Scale  Invariant Feature  Transform  (SIFT)  descriptor  instead  of  the  maxpooling  layer.  Max-pooling  layer  discards  the  pose,  i.e., translational and rotational relationship between the low-level features,  and  hence  unable  to  capture  the  spatial  hierarchies between low and high level features. The SIFT descriptor layer captures  the  orientation  and  the  spatial  relationship  of  the features extracted by convolutional layer. The proposed SIFT Descriptor  CNN  therefore  combines  the  feature  extraction capabilities  of  CNN  model  and  rotation  invariance  of  SIFT descriptor.  Experimental  results  on  the  MNIST  and fashionMNIST  datasets  indicates  reasonable  improvements over conventional methods available in literature.          
                        <p></p>
                        <a href="">paper link</a> | <a href="">pdf</a>
                        </a> </p>
                        </td>
                    </tr>
					
			<tr >
            
                    <td width="30%"><img id="img-opt" src="img/paper1.png" alt="project_img" width="160" style="border-style: none">
                    </td>
        
                        <td valign="top" width="70%">
                        <p><a href="https://ieeexplore.ieee.org/document/7249385">
            <papertitle>Hybrid Maximum Depth-kNN Method for Real-Time Node Tracking using Multi-Sensor Data</papertitle></a><br>
                        <em> IEEE International Conference on Communications (ICC) 2015, London, UK </em> <br> 
                        <p style="text-align:justify">In this paper, a hybrid MD-kNN method for real time sensor node tracking is proposed. The method combines two individual location hypothesis functions obtained from generalized maximum depth and generalized kNN methods. The individual location hypothesis functions are themselves obtained from multiple sensors measuring visible light, humidity, temperature, acoustics, and link quality. The hybrid MD-kNN method therefore combines the lower computational power of maximum depth and outlier rejection ability of kNN method to realize a robust real time localization method. Additionally, this method does not require the assumption of an underlying distribution under non-line-of-sight (NLOS) conditions. Additional novelty of this method is the utilization of multivariate data obtained from multiple sensors which has hitherto not been used. The affine invariance property of the hybrid MD-kNN method is proved and its robustness is illustrated in the context of node localization. Experimental results on the Intel Berkeley research data set indicates reasonable improvements over conventional methods available in literature.         
                        <p></p>
                        <a href="https://ieeexplore.ieee.org/document/7249385">paper link </a>  | <a href="files/paper1.pdf">pdf</a>
                        </a> </p>
                        </td>
                    </tr>
			
            <tr>
              <td>
                <heading style="font-size:22px">Notable Projects</heading>
              </td>
            </tr>

                  
            
            

            <tr >
        
                <td width="30%"><img id="img-opt" src="img/ee698m.png" alt="project_img" width="160" style="border-style: none">
                </td>
    
                  <td valign="top" width="70%">
                    <p><a href="files/EE698M_project_report.pdf">
      <papertitle>Direct Content Analysis for Scene Intensity Estimation in Movies using low-level multimodal features</papertitle></a><br>
                      <em>Supervisor: <a href="http://home.iitk.ac.in/~tanaya/Home.html">Dr. Tanaya Guha</a>, Indian Institute of Technology Kanpur</em><br>
                    <p style="text-align:justify">The project aims at developing a computational model to estimate scene intensity profile in movies or videos. Scene intensity can be understood as a measure of excitement or activity in a scene. 
                    <br><br>
                    •	Exploited computable video features namely, average shot length, color variance, motion content, lighting key, motion energy, harmonicity etc. As video features to compute scene intensity.
					<br>
					•	Incorporated facial emotion detection using the optical flow of facial interest points 
					<br>
					•	Created a small dataset by manually timestamping scene boundaries and conducted a survey asking people how critical they consider of these scenes in a particular movie. 
					<br>
					•	Various cinematic principles and video features is being exploited for robust scene intensity estimation.
					<br><br>
					Selected as the <b>best project</b> in the course comprising of over 40 students.
                    <p></p>
                    <a href="files/EE698M_project_report.pdf">report</a> | <a href="files/EE698M_Presentation.pdf">presentation</a>
                    </a> </p>
                  </td>
                </tr>

			<tr >
				<td width="30%"><img id="img-opt" src="img/ee629_convex.png" alt="project_img" width="160" style="border-style: none">
					</td>
					  <td valign="top" width="70%">
						<p><a href="files/EE609A_Convex.pdf">
		  <papertitle>Dictionary Learning and Sparse representation based Image Processing Applications </papertitle></a><br>
						  <em>Supervisor: <a href = "http://home.iitk.ac.in/~ketan/">Dr. Ketan Rajawat</a>, Indian Institute of Technology Kanpur</em><br>
						<p style="text-align:justify">The project aimed at exploring various dictionary learning algorithms(k-SVD, MOD, OMP)and implementing sparse representation based application in Image Processing like Image denoising, inpainting, classification, compression etc. 
						<br><br>
						•	Implemented image inpainting(removing corrupted pixels in the target region)using sparse representation on dictionary learned from randomly sampling patches from the source region of the image. <br>
						•	Compared Sparse based Image denoising using overcomplete DCT dictionary with state-of-art methods. <br>
						•	Implemented Sparse representation based Image classification on MNIST dataset. 
						<p></p>
						<a href="files/EE609A_Convex.pdf">report</a>
						</a> </p>
				  </td>
				</tr>

						
			<tr >
        
                <td width="30%"><img id="img-opt" src="img/ee698m.png" alt="project_img" width="160" style="border-style: none">
                </td>
    
                  <td valign="top" width="70%">
                    <p><a href="files/EE698M_project_report.pdf">
      <papertitle>Age and Gender Recognition of a Speaker from Interactive voice response (IVR)systems</papertitle></a><br>
                      <em>Supervisor: <a href="http://home.iitk.ac.in/~rhegde/">Dr R.M.Hegde</a>, Indian Institute of Technology Kanpur</em><br>
                    <p style="text-align:justify">The project aimed at building a system for Age and Gender Recognition using speech features.
                    <br><br>
                    •	Pre-processed and extracted useful long-term and short-term features including MFCC (Mel Frequency Cepstral Coefficients), Shifted Delta Cepstral (SDC), pitch, and first three formants information from the speech signals. 
					<br>
					•	Trained 128-mixture GMM model with MAP adaptation for MFCCs and used WSNMF for dimensionality reduction. 
					<br>
					•	Analysed performance of various machine learning classifiers such as Support Vector Machines (SVM), Random Forests, Decision Trees. 
					<br><br>
                    <p></p>
                    <a href="files/ee627_tp.pdf">report</a> | <a href="files/ee627_presentation.pdf">presentation</a>
                    </a> </p>
                  </td>
                </tr>
				
				
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td>
                  <heading style="font-size:22px">Internships</heading>
                </td>
              </tr>
              <tr >
                <td width="30%"><img id="img-pgp" src="img/samsung.jpeg" alt="project_img" width="160" style="border-style: none">
                    </td>
        
                      <td valign="top" width="70%">
                        <p><a href="http://www.samsung.com/in/aboutsamsung/samsungelectronics/india/rnd/">
          <papertitle>Samsung Research and Development Institute, Bengaluru (SRIB)</papertitle></a><br>
                          <em>Supervisor: Srinivas Rao Kudavelly, Principal Engineer, Innovation & Enterprise Biz Division/​HME(Health and Medical Equipments)/U​ltrasound) </em> <br>

                        <p style="text-align:justify">Pyramidal Implementation of Lucas-Kanade-Tomasi (LKT) Feature Tracking Algorithm for 3D Images
                        <br><br>
                        The project aimed at C++ implemention of Lucas-Kanade-Tomasi (LKT) Feature tracking algorithm. Pyramidal Implementation of the above algorithm has performance improvement in terms of local accuracy and robustness. <br>
						•	Tracked simple geometrical objects and its affine-transformed version with very high accuracy. <br>
						•	Used the algorithm for tracking real world scenarios (ultrasound 3D image volume) with reasonably high accuracy. <br>
						•	Analysed the sensitivity of algorithm to various parameters. <br>
							</p>
                      </td>
                    </tr>

                    


            <tr>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">inspired from</a>
                  <a href="http://www.cs.berkeley.edu/~barron/"> this website</a>
                  </font>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
    </table>
 <!-- Default Statcounter code for Personal Website
https://shrep.github.io/ -->
<script type="text/javascript">
var sc_project=11673319; 
var sc_invisible=1; 
var sc_security="327094c7"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics
Made Easy - StatCounter" href="http://statcounter.com/"
target="_blank"><img class="statcounter"
src="//c.statcounter.com/11673319/0/327094c7/1/" alt="Web
Analytics Made Easy - StatCounter"></a></div></noscript>
<!-- End of Statcounter Code -->
  </body>
</html>